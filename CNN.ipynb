{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Paths and Parameters\n",
    "train_data_dir = 'train/train'\n",
    "test_data_dir = 'test_images/Kaggle_test_images'\n",
    "img_size = (32, 32)\n",
    "batch_size = 32  # Smaller batch = better accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72000 images belonging to 10 classes.\n",
      "Found 18000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashar\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Custom CNN Model (Deeper and Better)\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashar\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 352ms/step - accuracy: 0.2481 - loss: 2.4031 - val_accuracy: 0.4024 - val_loss: 1.6901 - learning_rate: 5.0000e-04\n",
      "Epoch 2/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 334ms/step - accuracy: 0.3752 - loss: 1.6962 - val_accuracy: 0.3818 - val_loss: 1.6802 - learning_rate: 5.0000e-04\n",
      "Epoch 3/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 338ms/step - accuracy: 0.4448 - loss: 1.5062 - val_accuracy: 0.4978 - val_loss: 1.3651 - learning_rate: 5.0000e-04\n",
      "Epoch 4/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 338ms/step - accuracy: 0.4840 - loss: 1.4085 - val_accuracy: 0.5656 - val_loss: 1.2059 - learning_rate: 5.0000e-04\n",
      "Epoch 5/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 342ms/step - accuracy: 0.5155 - loss: 1.3268 - val_accuracy: 0.5997 - val_loss: 1.1181 - learning_rate: 5.0000e-04\n",
      "Epoch 6/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 342ms/step - accuracy: 0.5403 - loss: 1.2671 - val_accuracy: 0.6032 - val_loss: 1.1132 - learning_rate: 5.0000e-04\n",
      "Epoch 7/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 341ms/step - accuracy: 0.5591 - loss: 1.2180 - val_accuracy: 0.6232 - val_loss: 1.0685 - learning_rate: 5.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 341ms/step - accuracy: 0.5774 - loss: 1.1779 - val_accuracy: 0.6393 - val_loss: 1.0376 - learning_rate: 5.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 343ms/step - accuracy: 0.5917 - loss: 1.1375 - val_accuracy: 0.6754 - val_loss: 0.9474 - learning_rate: 5.0000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m785s\u001b[0m 349ms/step - accuracy: 0.6025 - loss: 1.1153 - val_accuracy: 0.6849 - val_loss: 0.9109 - learning_rate: 5.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m776s\u001b[0m 345ms/step - accuracy: 0.6096 - loss: 1.0938 - val_accuracy: 0.6414 - val_loss: 1.0451 - learning_rate: 5.0000e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 343ms/step - accuracy: 0.6214 - loss: 1.0671 - val_accuracy: 0.7001 - val_loss: 0.8540 - learning_rate: 5.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 343ms/step - accuracy: 0.6263 - loss: 1.0478 - val_accuracy: 0.6970 - val_loss: 0.8873 - learning_rate: 5.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 343ms/step - accuracy: 0.6328 - loss: 1.0316 - val_accuracy: 0.6998 - val_loss: 0.8892 - learning_rate: 5.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 343ms/step - accuracy: 0.6380 - loss: 1.0150 - val_accuracy: 0.7154 - val_loss: 0.8376 - learning_rate: 5.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 343ms/step - accuracy: 0.6409 - loss: 1.0052 - val_accuracy: 0.7343 - val_loss: 0.7827 - learning_rate: 5.0000e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 344ms/step - accuracy: 0.6481 - loss: 0.9897 - val_accuracy: 0.7393 - val_loss: 0.7696 - learning_rate: 5.0000e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 343ms/step - accuracy: 0.6482 - loss: 0.9895 - val_accuracy: 0.6653 - val_loss: 0.9929 - learning_rate: 5.0000e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 343ms/step - accuracy: 0.6556 - loss: 0.9716 - val_accuracy: 0.7366 - val_loss: 0.7764 - learning_rate: 5.0000e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 343ms/step - accuracy: 0.6575 - loss: 0.9608 - val_accuracy: 0.7350 - val_loss: 0.7939 - learning_rate: 5.0000e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6597 - loss: 0.9562\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m776s\u001b[0m 345ms/step - accuracy: 0.6597 - loss: 0.9562 - val_accuracy: 0.7262 - val_loss: 0.7979 - learning_rate: 5.0000e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m785s\u001b[0m 349ms/step - accuracy: 0.6702 - loss: 0.9238 - val_accuracy: 0.7626 - val_loss: 0.7068 - learning_rate: 2.5000e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m801s\u001b[0m 356ms/step - accuracy: 0.6819 - loss: 0.8949 - val_accuracy: 0.7688 - val_loss: 0.6862 - learning_rate: 2.5000e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 343ms/step - accuracy: 0.6893 - loss: 0.8764 - val_accuracy: 0.7558 - val_loss: 0.7156 - learning_rate: 2.5000e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 340ms/step - accuracy: 0.6891 - loss: 0.8731 - val_accuracy: 0.7732 - val_loss: 0.6729 - learning_rate: 2.5000e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 341ms/step - accuracy: 0.6920 - loss: 0.8672 - val_accuracy: 0.7643 - val_loss: 0.6918 - learning_rate: 2.5000e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 341ms/step - accuracy: 0.6930 - loss: 0.8679 - val_accuracy: 0.7584 - val_loss: 0.7045 - learning_rate: 2.5000e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 341ms/step - accuracy: 0.6934 - loss: 0.8605 - val_accuracy: 0.7744 - val_loss: 0.6708 - learning_rate: 2.5000e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 342ms/step - accuracy: 0.6942 - loss: 0.8580 - val_accuracy: 0.7615 - val_loss: 0.7102 - learning_rate: 2.5000e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 341ms/step - accuracy: 0.6934 - loss: 0.8625 - val_accuracy: 0.7571 - val_loss: 0.7161 - learning_rate: 2.5000e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 341ms/step - accuracy: 0.6974 - loss: 0.8541 - val_accuracy: 0.7819 - val_loss: 0.6434 - learning_rate: 2.5000e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 343ms/step - accuracy: 0.6992 - loss: 0.8453 - val_accuracy: 0.7779 - val_loss: 0.6529 - learning_rate: 2.5000e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 349ms/step - accuracy: 0.7005 - loss: 0.8424 - val_accuracy: 0.7657 - val_loss: 0.6845 - learning_rate: 2.5000e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m776s\u001b[0m 345ms/step - accuracy: 0.7006 - loss: 0.8422 - val_accuracy: 0.7797 - val_loss: 0.6497 - learning_rate: 2.5000e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.7050 - loss: 0.8317\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 345ms/step - accuracy: 0.7050 - loss: 0.8317 - val_accuracy: 0.7729 - val_loss: 0.6659 - learning_rate: 2.5000e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m778s\u001b[0m 346ms/step - accuracy: 0.7093 - loss: 0.8099 - val_accuracy: 0.7907 - val_loss: 0.6210 - learning_rate: 1.2500e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m781s\u001b[0m 347ms/step - accuracy: 0.7163 - loss: 0.8064 - val_accuracy: 0.7948 - val_loss: 0.6127 - learning_rate: 1.2500e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m785s\u001b[0m 349ms/step - accuracy: 0.7139 - loss: 0.8062 - val_accuracy: 0.7956 - val_loss: 0.6025 - learning_rate: 1.2500e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m784s\u001b[0m 348ms/step - accuracy: 0.7159 - loss: 0.8012 - val_accuracy: 0.7872 - val_loss: 0.6259 - learning_rate: 1.2500e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m784s\u001b[0m 348ms/step - accuracy: 0.7158 - loss: 0.7961 - val_accuracy: 0.7958 - val_loss: 0.5930 - learning_rate: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Compile and Train for 40 Epochs\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1),\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=40,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m563/563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 97ms/step - accuracy: 0.7922 - loss: 0.6077\n",
      "âœ… Final Validation Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Load Best Model\n",
    "model = load_model(\"best_model.keras\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_gen)\n",
    "print(f\"âœ… Final Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images loaded: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Load Test Images\n",
    "test_images = []\n",
    "test_ids = []\n",
    "\n",
    "for filename in sorted(os.listdir(test_data_dir)):\n",
    "    path = os.path.join(test_data_dir, filename)\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img.astype('float32') / 255.0\n",
    "    test_images.append(img)\n",
    "    test_ids.append(filename)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "print(\"Test images loaded:\", test_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 77ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Predict on Test Images\n",
    "preds = model.predict(test_images)\n",
    "pred_labels = np.argmax(preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Submission saved as: Ashar_CV5_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Save Prediction CSV\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"label\": pred_labels\n",
    "})\n",
    "submission.to_csv(\"Ashar_CV5_submission.csv\", index=False)\n",
    "print(\"ğŸ¯ Submission saved as: Ashar_CV5_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
